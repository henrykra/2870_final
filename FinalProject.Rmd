---
title: "Final Project CS 2870"
author: "Skyler Heininger, Andy English, Henry Kraessig"
date: "11/29/2023"
output: html_document
---

## Set up
```{r setup, message = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center",
                      warning = F,
                      message = F)
pacman::p_load(rpart, rpart.plot, tidyverse, caret)

theme_set(theme_bw())
```

# STAT/CS 2870 Final Project
Skyler Heininger, Henry Kraessig, Andy English

## I. Introduction
	
This project focused on analyzing MLB pitch data from 2018. This data was generated by the MLB from sensor data during MLB games in 2018. This data is a sample because it is all games within the MLB during the 2018 season. Any other teams not in the MLB are not included. This could provide some sort of sampling bias towards the average pitch within this dataset being “better” than the average pitch in general, due to the professional level of the MLB. However, if we are just analyzing this data based on the context of the MLB in 2018, then there is likely no sampling bias since this dataset contains all pitches thrown within the 2018 season. Based on these assumptions, there is the possibility of bias when comparing this with other seasons or all-time due to it only being the 2018 season, where there could be different strategies in play compared to other seasons. As such, our analysis of the data is limited to the 2018 season and does not make generalizations about all-time pitching data. This is clearly an observational study since it is gathering data from an already-established practice without manipulating any variables intentionally. The measurements taken were done using sensors. To handle bias, we assume the measurements were taken the same for every single pitch, at every game, at every stadium during the 2018 MLB season. Really, we are treating the measurements taken as being purely objective and performed in the same manner for every pitch. As such, we do not anticipate any bias within the measurements. This dataset is extremely interesting due to the pure amount of data present about each pitch and how a pitcher could be classified for applications in scouting, player evaluation, and player development. Finally, data cleaning was necessary to achieve a usable dataset. This was because the data came in the form of a 2015-2018 dataset, which was extremely large and took long periods of time to run. As such, this dataset was filtered down to a 2018 dataset. The following code chunk displays the code necessary for processing this data.

```{r data loading and cleaning}

# # Load all data relevant to the pitches
# pitches <- read.csv("data/pitches.csv")
# abs <- read.csv("data/atbats.csv")
# names <- read.csv("data/player_names.csv")
# games <- read.csv("data/games.csv")
# 
# # Join pitches and at bat, filter for 2018
# p_2018 <-
#   pitches |>
#   left_join(
#     y = abs,
#     by = "ab_id"
#   ) |>
#   filter(
#     g_id >= 201800000
#   )
# 
# # Join pitches/atbat with batter names dataset
# p_2018_names <-
#   p_2018 |>
#   left_join(
#     y = names,
#     by = c("batter_id" = "id"),
#     keep = F
#   )
# 
# # Make full batter name using first name and last name
# p_2018_names_2 <-
#   p_2018_names |>
#   mutate(batter_name = paste(first_name, last_name))
# 
# # join pitcher names into dataset
# p_2018_names_3 <-
#   p_2018_names_2 |>
#   left_join(
#     y = names,
#     by = c("pitcher_id" = "id"),
#     keep = F
#   )
# 
# # Make full name for pitchers, remove
# # individual first and last names
# pitches_2018_full <-
#   p_2018_names_3 |>
#   mutate(pitcher_name = paste(first_name.y,
#                               last_name.y)) |>
#   select(-first_name.x, -first_name.y,
#          -last_name.x, -last_name.y)
# 
# # Save dataset
# write.csv(pitches_2018_full, "data/2018_pitches_full.csv",
#           row.names = F,
#           quote = F)
pitches <- read.csv("2018_pitches_full.csv")


```

## II. Data Visualizations

One of the relationships we wanted to pursue was the differences in spin rate and angle break between different pitch types. Spin rate is how fast the ball is spinning around its axis, in rotations per minute. The angle break is the angle in degrees of deviation from the theoretical path of the baseball, if it continued to fly in a directly straight line.


```{r spin break}
# Clean out unused pitch types
pitches_spin_cleaned <- pitches |>
  mutate(pitch_type = ifelse(pitch_type == "FO","PO", pitch_type)) |>
  filter(!pitch_type %in% c("", "AB","EP","PO"))

# Get summary statistics of the various variables
summary_stats <- pitches_spin_cleaned |>
  summarise(across(c(spin_rate, break_angle),
                   list(mean = mean, sd = sd)))

# remove any data points more than 3 standard deviations from the mean
threshold <- 3
pitches_filtered <- pitches_spin_cleaned |>
  filter(
    abs(spin_rate - summary_stats$spin_rate_mean) <= threshold * summary_stats$spin_rate_sd,
    abs(break_angle - summary_stats$break_angle_mean) <= threshold * summary_stats$break_angle_sd
  )

renamed_pitches_clean <- pitches_filtered |>
  mutate(updated_pitch_type = recode(
    pitch_type,
    "CH" = "Changeup",
    "CU" = "Curveball",
    "FC" = "Cutter",
    "FF" = "Four-seam Fastball",
    "FS" = "Splitter",
    "FT" = "Two-seam Fastball",
    "KC" = "Knuckle curve",
    "KN" = "Knuckleball",
    "SCy" = "Screwball",
    "SI" = "Sinker",
    "SL" = "Slider"
  ))

# Plotting the spin rate with break angle, by each pitch type - This could be interesting to include since it contains a neat graph: the amount of angle break is bounded by what is possible. As shown above, this is mirrored in horizontal movement during flight, and somewhat vertical movement (pfx_x and pfx_z, respectively)
ggplot(data = renamed_pitches_clean, mapping = aes(x = spin_rate, y = break_angle, color = updated_pitch_type)) +
  geom_point(show.legend = F, alpha = 0.15) + # Low alpha to allow you to see densities of points
  facet_wrap(facets = ~updated_pitch_type) +
  labs(x = "Spin Rate (RPM)", y = "Break Angle (Degrees)", # need double check on degrees
       title = "Spin Rate and Break Angle for Each Pitch Type") + 
  theme(plot.title = element_text(hjust = 0.5))

```

The above figure has several interesting aspects. To start, every graph shows bounds along the upper and lower sides, with the same cone-shape. This is due to a sort of maximum angle break possible per a given spin rate. Across all pitches, this varies slightly, but every pitch type follows this similar cone shape, indicating a threshold for angle break given a spin rate. To note, the presence of the negative break angle axis comes from left and right handed pitchers, changing how the ball flies through the air. 

Past simply spin rate and angle break, the shapes of each pitch type varies, slightly in some cases, and more significantly in other cases. For instance, a Knuckleball will not exceed a spin rate of 2200, based on this dataset. As such, knuckleballs also do not reach higher angle breaks. There are also noteworthy scenarios in the data where at higher spin rates, smaller angle breaks aren’t possible. This occurs for Changeups, Splitters, Sinkers, Two-seam Fastballs, Four-seam Fastballs, and other pitch types, to varying degrees. Really, the higher the spin rate, across most pitch types, the fewer the pitches that have low angle breaks. The only exceptions are the Four-seam Fastball and the Cutter, which both have angle breaks of 0° at spin rates around 2500 rpm. This clearly shows different behaviors of the ball during flight for different pitch types, although there are more differences between some pitch types than others. In conclusion for this data visualization, there are distinct differences in angle break and spin rate mechanics based on the pitch type. To prove this, the next code chunk displays ANOVA tests of spin rate and break angle across pitch types, both of which are statistically significant. This indicates that both the spin rate and break angle of each pitch type is unique.

```{r anova tests and summary statistics}
# Means for each pitch type for spin rate and break rate
renamed_pitches_clean |>
  group_by(pitch_type) |>
  summarize(avg_break_angle = mean(break_angle),
            avg_spin_rate = mean(spin_rate)) ->
  avgs

head(avgs, 11)

# Anova tests
anova_spin <- aov(spin_rate ~ pitch_type, data = pitches)
anova_break <- aov(break_angle ~ pitch_type, data = pitches)
summary(anova_spin)
summary(anova_break)

```



One of the unique aspects of our dataset is the exact coordinates of each pitch as it crosses home plate. We were curious about pitcher strategy when it comes to the locations they aim for. When observing the location of all pitches from the 2018 season, the pitches tend to be most densely clustered around the center of the strike zone. When filtering for the pitchers throwing hand and the batter's stance, the data becomes much more indicative of pitcher strategy. The figures below show the normalized density of pitch location, where the lighter coolers indicate a higher density of pitches at that location. 

```{r first heat map}



```


The data shows that all pitchers generally prefer to throw to the batters outside, no matter which side of the plate the batter lines up on. The pitcher favored matchups, where the pitchers throwing hand is the same as the batter’s stance, have noticeably different distributions compared to the batter favored matchups. In the pitcher favored matchups, the top of the zone has more pitches on the pitcher’s arm side, and at the lower areas of the plate, the pitches tail off to the glove side. 

We were also curious to see how pitch location is affected by pitch type. 


```{r second heat map}




```

The distributions show 4 - seam fastballs as the pitch that generally occupies the top half of the strike zone, and sliders, curveballs, and changeups occupy the lower parts of the zone. The location of sliders is very right biased, which makes sense based on the nature of the pitch. Sliders in the modern MLB usually have a lot of glove-side movement, and many more pitches are thrown by right-handers, which explains why the slider tends to enter the zone on the right half of the plate. I predict that when splitting the data between left-handed and right-handed pitchers, the higher density areas for sliders would be on opposite sides of home plate. 


```{r pie chart replacement 1}



```

We wanted to visualize how batters fared against the 25 ‘elite’ pitchers selected (15 RHP and 10 LHP), who were selected based on 2018 pitching leaderboards and all-star teams. We removed the 13 rarest outcomes for an at-bat, including catcher’s interference, strikeout double play, runner out, among others. However, the pie chart still includes pieces that are barely visible, like triple, sac fly, field error, and hit by pitch which we may end up removing to make it easier for the viewer to interpret. Additionally, we filtered out only the events that are deemed ‘productive’ for the batter, where either they reach base themselves or sacrifice to advance a runner. We can see that elite pitchers limit extra base hits very well, over ¾ of the productive results for a batter is either a walk or a single. This is unsurprising, as teams rely on their aces to pitch lots of innings and they don’t want to be beat on just a single pitch. 


```{r pie chart replacement 2}



```


## III. Machine Learning Methods

```{r strikeout model code chunk and graph}



```

This is a basic regression model where it predicted the probability of a strikeout based on start speed, spin rate, and nasty (a measure of how hard a pitch is to hit from 0-100). The nasty variable tends to follow a normal distribution. For the final, we will make a more complex regression model, potentially based on strikeout probability, called strike vs called ball, or something similar. The graph does not display the data exactly how I intended, it is hard to see the exact proportion of strikeout vs no K. But you can see as the predicted probability of a K rises, particularly above .35 the green bar indicating an actual strikeout is getting closer and closer to the red bar, indicating that the model is at least somewhat useful. 


We developed a classification tree to determine the pitch type. To do this, we first removed variables from the data that had to do with the game state. This was because the high number of variables and large amount of data made it take lengthy amounts of time. As such, we decided to reduce the number of columns. First, we removed all Id and Name columns from the data, as these were specific to games, pitchers, and batters. The presence of these as categorical variables of many many types would have made calculation of a classification tree both large and very specific to the batters and pitchers in the dataset. Additionally, things like the game id were simply unnecessary. The following code chunk shows the data cleaning necessary to remove all the columns. Then, we decided to shift the goal of this classification tree to determine the pitch type solely based on data having to do with the sensor data, such as positions, velocities, spin rates, etc. This eliminated even more variables, leaving 25 variables having to do with pitch sensor data.

```{r additional cleaning}
# Select pitch types and change to factor types
pitches_filtered <- pitches |>
  select(-c(pitcher_name, batter_name, pitcher_id, g_id, batter_id, ab_id)) |>
  filter(!pitch_type %in% c("", "AB","EP","PO", "FO")) |>
  mutate(code = as.factor(code),
         type = as.factor(code),
         zone = as.factor(zone),
         pitch_type = as.factor(pitch_type),
         across(where(is.character), as.factor))

# determine which columns to remove
pitches_filtered |>
  select(where(is.factor)) |>
  colnames() ->
  to_remove

to_remove <- to_remove[to_remove != "pitch_type"]

pitches_filtered2 <- pitches_filtered |>
  # The following select removes all unnecessary variables (unnecessary having to do with anything but the pitch itself)
  select(-all_of(to_remove), # This keeps pitch_type, which is needed (Rstudio recommended all_of)
         -c(inning, p_score, o, outs, on_1b, on_2b, on_3b, outs, 
            event_num, outs, b_score, b_count, s_count, type_confidence, pitch_num))

```


After this was performed, we constructed a classification tree using Rpart. Then, we pruned the tree, determined variables of importance, and calculated the accuracy of the tree. The next code chunk contains all of this and the variable importance graph below shows the variables of importance. Unfortunately, the tree does have 4569 leaf nodes, which makes the resulting graph far too large to graph. While this seems very large for a classification tree, the unpruned version had 68571 leaf nodes, making the pruned version a fraction of the size.

```{r full classification tree and pruning}

# Build the full decision tree here (This will take a while to do, mine took around 15 minutes)
pitch_full_tree <- rpart(
  formula = pitch_type ~ .,   # explanatory and response variables
  data = pitches_filtered2,
  method = "class",
  parms = list(split = "information"), # Using entropy
  minsplit = 0,
  minbucket = 0,
  cp = -1
)


pitch_full_tree$cptable |> 
  data.frame() |>
  # finding row with smallest xerror
  slice_min(xerror, n=1, with_ties = F) |>
  # create xerror cutoff = xerror + xstd
  mutate(xerror_cutoff = xerror + xstd) |>
  # picking xerror_cutoff table
  pull(xerror_cutoff) ->   # Saves as vector, only give one column
  xcutoff

# Finding the "best" tree using xcutoff
pitch_full_tree$cptable |> 
  data.frame() |>
  # Keeping all rows below xcutoff
  filter(xerror < xcutoff) |>      
  # Get simplest one
  slice(1)

# Finding the "best" tree using xcutoff
pitch_full_tree$cptable |> 
  data.frame() |>
  # Keeping all rows below xcutoff
  filter(xerror < xcutoff) |>  
  # Get simplest one
  slice(1) |>
  # Picking cp value out of dataframe 
  pull(CP) ->
  cp_prune

c("xerror cutoff" = xcutoff,
  "cp prune value" = cp_prune)

pitches_pruned <- prune(tree = pitch_full_tree,
                     cp = cp_prune)

```

```{r variables of importance}

# This code is taken directly from class, plot importance of variables
caret::varImp(pitches_pruned) |> 
  arrange(desc(Overall)) |> 
  rownames_to_column(var = "variable") |> 
  
  ggplot(mapping = aes(x = fct_reorder(variable, -Overall),
                       y = Overall)) + 
  
  geom_col(fill = "steelblue",
           color = "black") + 
  
  labs(x = NULL,
       y = "Variable Importance",
       title = "Variable Importance in Pruned Classification Tree for Pitch type data") + 
  
  scale_y_continuous(expand = c(0, 0, 0.05, 0)) +
  
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```

```{r prediction and accuracies}

predict(
  object = pitches_pruned,
  newdata = pitches_filtered2,
  type = "class"
) ->
  pitches_predicted

# Creating the confusion matrix:
confusionMatrix(
  data = pitches_predicted,
  reference = pitches_filtered2$pitch_type
)

```

Based on these figures, especially the variable importance figure, we decided to try moving forward with reducing the complexity of the classification tree by removing unimportant variables. This entailed removing the variables; strikezone top and bottom, break in the y direction, and starting y position. This increased the number of splits in the classification tree from 4568 to 6886, with an overall accuracy increase from 86.46% to 87.59%. This was done using the same pipeline as outlined within the previous three code chunks, with the only difference being removing the 4 least important variables from the data. The drastic increase in complexity accompanying this change, while hardly increasing accuracy was determined not to be useful. As such, the final classification tree for determining pitch type uses the variables shown in the variable importance graph. The importance of these variables is very interesting to see, with break length, start speed, velocity in the y direction, acceleration in the z direction, and end speed are the five most important variables. Close after this is spin rate, which makes sense as a useful variable for classification based on our prior data analysis. Break angle, another variable we had analyzed previously, was less useful, but useful nonetheless. 



We then developed a KNN model to determine the result of a given pitch, once again based on sensor data. The possible outcomes included a ball, ball in the dirt, swinging strike, called strike, foul, foul tip, intentional ball, and more. The pipeline for this followed a similar path to before of removing categorical data, except for code and pitch type, and then filtering for a single pitch type, Knuckle Curve This was because running a KNN model on the entire dataset would have taken far too long upwards of 10 minutes. Determining the best choice of K with this in mind would've taken much longer. Because of this, we determined that the Knuckle Curve was a good pitch type to try to predict the outcome of since there were 16327 data entries. This is a pretty decent amount of data, but considering there are 13 levels, with varying levels in each group, the Knuckle curve did a good job of providing enough data points for KNN to work.

```{r pitch types}
# Show the amounts of each pitch type
table(pitches$pitch_type)

```

We proceed with data cleaning as discussed.

```{r clean for knn}

# Need to remove batter name and pitcher name, and all ids 
# also need to filter out bad pitch data
pitches_filtered_knn <- pitches |>
  select(-c(pitcher_name, batter_name, pitcher_id, g_id, batter_id, ab_id)) |>
  filter(!pitch_type %in% c("", "AB","EP","PO", "FO")) |>
  mutate(code = as.factor(code),
         type = as.factor(code),
         zone = as.factor(zone),
         pitch_type = as.factor(pitch_type),
         across(where(is.character), as.factor)) |>
  filter(pitch_type == "KC") 


# Remove factor columns (can't have these in KNN)
pitches_filtered_knn |>
  select(where(is.factor)) |>
  colnames() ->
  to_remove

to_remove <- to_remove[to_remove != "code"]
  # The following select removes all unnecessary variables (unnecessary having to do with anything but the pitch itself)
pitches_filtered_knn2 <- pitches_filtered_knn |>
  select(-all_of(to_remove), # This keeps pitch_type, which is needed (Rstudio recommended all_of)
         -c(inning, p_score, o, outs, on_1b, on_2b, on_3b, outs, 
            event_num, outs, b_score, b_count, s_count, type_confidence, pitch_num))

# Make sure correct columns are removed
tibble(head(pitches_filtered_knn2))
# unique(pitches_filtered_knn2$code) # Use this to check the codes

```


Since KNN needs normalized or standardized data, we need to standardize and normalize the data.

```{r normalize and standardize data}
# normalize function
normalize <- 
  function(x){
  return((x - min(x))/(max(x) - min(x)))
}

# Normalizing the cat data:
knn_norm <- 
  pitches_filtered_knn2 |> 
  mutate(across(.cols = where(is.numeric),
                .fns = normalize))

# Checking that the min is 0 and max is 1 for both columns:
knn_norm |> 
  summarize(across(.cols = where(is.numeric),
                   .fns = list(min = min, 
                               max = max)))

# standardize function
standardize <- 
  function(x){
    return((x - mean(x))/sd(x))
  }

# standardizing the cat data:
knn_stan <- 
  pitches_filtered_knn2 |> 
  mutate(across(.cols = where(is.numeric),
                .fns = standardize))

# Checking that the mean is 0 and the standard deviation is 1 for both columns:
knn_stan |> 
  summarize(across(.cols = where(is.numeric),
                   .fns = list(avg = mean, 
                               sd = sd))) |> 
  round(digits = 5)

```

Considering there is still a large amount of data, we will be using partitioning. LOOCV/the holdout method would also work, but would probably take longer to run. Additionally, we randomly sample from the dataset in order to not introduce any form of bias towards the earlier season or later season in 2018.

```{r train test split}
RNGversion("4.1.0")
set.seed(2870)
# Perform train test split to separate the data
# From https://www.statology.org/train-test-split-r/
sample <- sample(c(TRUE, FALSE), nrow(pitches_filtered_knn), replace=TRUE, prob=c(0.8,0.2))
train_stan <- knn_stan[sample, ]
test_stan <- knn_stan[!sample, ]
train_norm <- knn_norm[sample, ]
test_norm <- knn_norm[!sample, ]


```


Next, we perform a K search, looking for the best accuracy across the standardized and normalized data for several selections of k. We unfortunately cannot test every single possible selection of k, since this would be computationally expensive and we do not have such resources at our disposal. Because of this, a range of ks has been chosen that outlines the maximum accuracy found for k, through the searching of ks from 5 to 500. These won't all be included in the RMD, as a result of the time this would take to do.

```{r knn choosing k}
RNGversion("4.1.0")
set.seed(2870)

# It turns out that using all columns for training isn't too long in run time
knn_cols <- train_norm |> select(where(is.numeric)) |> colnames()

# Data frame for knn accuracy - if the chosen value is 100, may need to increase upper bound
knn_acc <- 
  tibble(
    k = 10:100, # this will take a while but is necessary for the graph below
    norm_acc = rep(-1, length(k)),
    stan_acc = rep(-1, length(k))
  ) 

for (i in 1:nrow(knn_acc)) {
  # Use normalized data
  # Fitting knn algorithm for ith iteration of loop
  loop_norm <- knn(train = train_norm[, knn_cols],
                  test = test_norm[, knn_cols],
                  cl = train_norm$code,
                  k = knn_acc$k[i])
  
  # forming confusion matrix for ith iteration of normalized data
  loop_cm_norm <- confusionMatrix(
    data = loop_norm, 
    reference = test_norm$code
  )
  
  # store accuracy
  knn_acc[i, 2] <- loop_cm_norm$overall[1]
  
  # Use standardized data
  # Fitting knn algorithm for ith iteration of loop
  loop_stan <- knn(train = train_stan[, knn_cols],
                  test = test_stan[, knn_cols],
                  cl = train_stan$code,
                  k = knn_acc$k[i])
  
  # forming confusion matrix for ith iteration of normalized data
  loop_cm_stan <- confusionMatrix(
    data = loop_stan, 
    reference = test_stan$code
  )
  
  # same as confusion matrix
  knn_acc[i, 3] <- loop_cm_stan$overall[1]
}

knn_acc

knn_acc |>
  pivot_longer(cols = -k,
               names_to = "rescale_method",
               values_to = "accuracy") |> 
  slice_max(accuracy)

```

Finally, below is a graph of K with corresponding accuracies. As we can see, for predicting the outcome of pitches in the 2018 MLB data set, 47.71% of pitches are correctly identified using KNN on sensor data. While this accuracy does seem low, there are factors to take into account that we couldn't do so in our data. Each pitcher and batter are different, and the state of the game may have influence on the performance on either of them. As such, this approach of using just sensor data may no be the greatest approach, but KNN also cannot use the categorical data necessary to describe the game state. Because of this, KNN is not the optimal method for determining outcome, but it is still better than nothing, as given by the following code chunk.

```{r best accuracy}
# Checking the results using resubstitution
predicted_norm <- knn(train = train_stan[, knn_cols],
                  test = test_stan[, knn_cols],
                  cl = train_stan$code,
                  k = 57)

confusionMatrix(
    data = predicted_norm, 
    reference = test_stan$code
  )


```

Even though this is a poor overall accuracy, it is still statistically significant over the no information rate. It is also noteworthy that Classes D, E, H, L, M, and T all have no predictions. This is because these aren't present in the data of knuckleballs, so no predictions can be made using this. Even though this would be nice to be avoided, increasing our data set to the next highest pitch type, FC (Cutter), more than doubles the size of the data that is being predicted. This is a large problem because of run time, as searching for k in the above data already takes upwards of 20 minutes.

This is also taking into account the best possible choice of K, being 57. This means that to determine the outcome of a single point, with 47.71% accuracy, we can determine the outcome of a pitch. This being the case, we can see the accuracies for given ks from 10 to 100 in the following graph.

```{r accuracies per k}
# Plot k
knn_acc |> 
  pivot_longer(cols = -k,
               names_to = "rescale_method",
               values_to = "accuracy") |> 
  ggplot(mapping = aes(x=k, y=accuracy)) +
  geom_line(mapping = aes(color = rescale_method)) +
  labs(title = "KNN accuracy over choice Ks",
       x = "Choice of k",
       y = "Accuracy")
  
```


This graph is very interesting since standardized accuracy is much higher than normalized accuracy. It also looks like the accuracy begins to trend upwards with increasing k. This is not the case, as given in the following graph.

```{r knn showing max k}
RNGversion("4.1.0")
set.seed(2870)

# It turns out that using all columns for training isn't too long in run time
knn_cols <- train_norm |> select(where(is.numeric)) |> colnames()

# Data frame for knn accuracy - if the chosen value is 100, may need to increase upper bound
knn_acc <- 
  tibble(
    k = seq(from = 11, to = 411, by = 20), # this will take a while but is necessary for the graph below
    norm_acc = rep(-1, length(k)),
    stan_acc = rep(-1, length(k))
  ) 

for (i in 1:nrow(knn_acc)) {
  # Use normalized data
  # Fitting knn algorithm for ith iteration of loop
  loop_norm <- knn(train = train_norm[, knn_cols],
                  test = test_norm[, knn_cols],
                  cl = train_norm$code,
                  k = knn_acc$k[i])
  
  # forming confusion matrix for ith iteration of normalized data
  loop_cm_norm <- confusionMatrix(
    data = loop_norm, 
    reference = test_norm$code
  )
  
  # store accuracy
  knn_acc[i, 2] <- loop_cm_norm$overall[1]
  
  # Use standardized data
  # Fitting knn algorithm for ith iteration of loop
  loop_stan <- knn(train = train_stan[, knn_cols],
                  test = test_stan[, knn_cols],
                  cl = train_stan$code,
                  k = knn_acc$k[i])
  
  # forming confusion matrix for ith iteration of normalized data
  loop_cm_stan <- confusionMatrix(
    data = loop_stan, 
    reference = test_stan$code
  )
  
  # same as confusion matrix
  knn_acc[i, 3] <- loop_cm_stan$overall[1]
}

# Plot k
knn_acc |> 
  pivot_longer(cols = -k,
               names_to = "rescale_method",
               values_to = "accuracy") |> 
  ggplot(mapping = aes(x=k, y=accuracy)) +
  geom_line(mapping = aes(color = rescale_method)) +
  labs(title = "KNN accuracy over choice Ks",
       x = "Choice of k",
       y = "Accuracy")

```

This clearly indicates that we have our maximum choice of k. Increasing k even further past this point quickly runs into run time errors. With the downward trend of accuracy with increasing k with the range in the above graph, we determined that this was sufficient evidence for our choice of k






## IV. Conclusions

In this study, we analyzed many variables having to do with pitches in the 2018 MLB pitching dataset. To start, we determined significant differences in spin rate and break rate based on the type of the pitch. We found that all pitches share some characteristics, such as maximum possible break angles per spin rate, but some pitches vary in their other characteristics. For instance, some pitches don’t reach higher spin rates or some pitches don’t have lower break angles at high spin rates. This provided enough information to pursue a classification tree based on spin rate, break angle, and many other possible variables.

Our classification tree was able to determine the type of pitch based on sensor data with 86.46% accuracy. While this tree is arguably complex, having 4568 leaf nodes, there are also over 700,000 data entries and 25 variables being used for classification. This classification tree was also much better at predicting pitch types than simply choosing the most common pitch type, whose accuracy was 35.39%. The resulting variables of importance within the classification tree also were interesting, with break length, start speed, velocity in the y direction, acceleration in the z direction, and end speed being the five most important variables. The variable importance graph outlines this very well.

One particular aspect we were interested in was where the ‘elite’ pitchers stand out compared to the rest of their peers. We manually selected 25 starting pitchers, 15 RHP and 10 LHP, based on the 2018 league leaders & all-stars. One thing done to compare elite pitchers to everyone else was filtering pitch type to only the 3 kinds of fastballs (4-seam, 2-seam, and cutter) and calculating average velocity compared to the rest of the dataset, which unsurprisingly ended up higher. 

The conversation around what makes a great MLB pitcher often revolves around the concept of “command”. Having good command over a pitch essentially means that it goes where the pitcher wants it to. In the analysis of pitch locations for all the pitches in the 2018 season, it starts to reveal a general league strategy based on a couple of factors that we analyzed, namely pitch type and dominant hand. These insights allow for a more exact criterion for what it takes for a pitcher to have command over their pitches. Pitchers in the MLB are usually judged by their outcome stats, like the amount of hits or runs they give up, which are dependent on many other factors of the game. But pitch location is almost completely up to the pitcher. So if a pitcher can keep their fastballs in the upper half of the strike zone, and keep sliders away from the middle of the plate, it can give a more quantitative view of their command. 

## V. Limitations / Recommendations

One way this study was limited was in time and scope. The dataset used has many more variables and relationships between variables than were used within this study. There are many more things to pursue in this project. For instance, as mentioned in our proposal, “what parts of the zone are the lowest xBA for breaking balls & offspeed pitches?” This was one question we did not have time to address and is a possible future use case. 

Another limitation of this project was in terms of computational power. For the classification tree machine learning method, we ended up using half the variables that we had access to. To be fair, there were several variables that wouldn’t have been useful for this step, such as game id, but others would have possibly been useful. With the large amount of data present, it took around 20 to 30 minutes to create a full classification tree. This drastically cut down how extensively we could explore different possible classification trees, as each run would take a significant amount of time. 




