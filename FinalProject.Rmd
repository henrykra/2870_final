---
title: "Final Project CS 2870"
author: "Skyler Heininger, Andy English, Henry Kraessig"
date: "11/29/2023"
output: html_document
---

## Set up
```{r setup, message = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center",
                      warning = F,
                      message = F)
pacman::p_load(rpart, rpart.plot, tidyverse, caret, scales, GGally, broom, class) 

theme_set(theme_bw())
```

# STAT/CS 2870 Final Project
Skyler Heininger, Henry Kraessig, Andy English

## I. Introduction
	
This project focused on analyzing MLB pitch data from 2018. This data was generated by the MLB from sensor data during MLB games in 2018. This data is a sample because it is all games within the MLB during the 2018 season. Any other teams not in the MLB are not included. This could provide some sort of sampling bias towards the average pitch within this dataset being “better” than the average pitch in general, due to the professional level of the MLB. However, if we are just analyzing this data based on the context of the MLB in 2018, then there is likely no sampling bias since this dataset contains all pitches thrown within the 2018 season. Based on these assumptions, there is the possibility of bias when comparing this with other seasons or all-time due to it only being the 2018 season, where there could be different strategies in play compared to other seasons. As such, our analysis of the data is limited to the 2018 season and does not make generalizations about all-time pitching data. This is clearly an observational study since it is gathering data from an already-established practice without manipulating any variables intentionally. The measurements taken were done using sensors. To handle bias, we assume the measurements were taken the same for every single pitch, at every game, at every stadium during the 2018 MLB season. Really, we are treating the measurements taken as being purely objective and performed in the same manner for every pitch. As such, we do not anticipate any bias within the measurements. This dataset is extremely interesting due to the pure amount of data present about each pitch and how a pitcher could be classified for applications in scouting, player evaluation, and player development. Finally, data cleaning was necessary to achieve a usable dataset. This was because the data came in the form of a 2015-2018 dataset, which was extremely large and took long periods of time to run. As such, this dataset was filtered down to a 2018 dataset. The following code chunk displays the code necessary for processing this data.

```{r data loading and cleaning, eval=FALSE}

# Load all data relevant to the pitches
pitches <- read.csv("data/pitches.csv")
abs <- read.csv("data/atbats.csv")
names <- read.csv("data/player_names.csv")
games <- read.csv("data/games.csv")

# Join pitches and at bat, filter for 2018
p_2018 <-
  pitches |>
  left_join(
    y = abs,
    by = "ab_id"
  ) |>
  filter(
    g_id >= 201800000
  )

# Join pitches/atbat with batter names dataset
p_2018_names <-
  p_2018 |>
  left_join(
    y = names,
    by = c("batter_id" = "id"),
    keep = F
  )

# Make full batter name using first name and last name
p_2018_names_2 <-
  p_2018_names |>
  mutate(batter_name = paste(first_name, last_name))

# join pitcher names into dataset
p_2018_names_3 <-
  p_2018_names_2 |>
  left_join(
    y = names,
    by = c("pitcher_id" = "id"),
    keep = F
  )

# Make full name for pitchers, remove
# individual first and last names
pitches_2018_full <-
  p_2018_names_3 |>
  mutate(pitcher_name = paste(first_name.y,
                              last_name.y)) |>
  select(-first_name.x, -first_name.y,
         -last_name.x, -last_name.y)

# Save dataset
write.csv(pitches_2018_full, "2018_pitches_full.csv",
          row.names = F,
          quote = F)

```


The above code chunk was only executed once, as this step takes time to do. Since we saved the new data to a csv, we can now load it.

```{r Loading data from csv}

pitches <- read.csv("2018_pitches_full.csv")

```


## II. Data Visualizations

One of the relationships we wanted to pursue was the differences in spin rate and angle break between different pitch types. Spin rate is how fast the ball is spinning around its axis, in rotations per minute. The angle break is the angle in degrees of deviation from the theoretical path of the baseball, if it continued to fly in a directly straight line.


```{r spin break}
# Clean out unused pitch types
pitches_spin_cleaned <- pitches |>
  mutate(pitch_type = ifelse(pitch_type == "FO","PO", pitch_type)) |>
  filter(!pitch_type %in% c("", "AB","EP","PO"))

# Get summary statistics of the various variables
summary_stats <- pitches_spin_cleaned |>
  summarise(across(c(spin_rate, break_angle),
                   list(mean = mean, sd = sd)))

# remove any data points more than 3 standard deviations from the mean
threshold <- 3
pitches_filtered <- pitches_spin_cleaned |>
  filter(
    abs(spin_rate - summary_stats$spin_rate_mean) <= threshold * summary_stats$spin_rate_sd,
    abs(break_angle - summary_stats$break_angle_mean) <= threshold * summary_stats$break_angle_sd
  )

renamed_pitches_clean <- pitches_filtered |>
  mutate(updated_pitch_type = recode(
    pitch_type,
    "CH" = "Changeup",
    "CU" = "Curveball",
    "FC" = "Cutter",
    "FF" = "Four-seam Fastball",
    "FS" = "Splitter",
    "FT" = "Two-seam Fastball",
    "KC" = "Knuckle curve",
    "KN" = "Knuckleball",
    "SCy" = "Screwball",
    "SI" = "Sinker",
    "SL" = "Slider"
  ))

# Plotting the spin rate with break angle, by each pitch type - This could be interesting to include since it contains a neat graph: the amount of angle break is bounded by what is possible. As shown above, this is mirrored in horizontal movement during flight, and somewhat vertical movement (pfx_x and pfx_z, respectively)
ggplot(data = renamed_pitches_clean, mapping = aes(x = spin_rate, y = break_angle, color = updated_pitch_type)) +
  geom_point(show.legend = F, alpha = 0.15) + # Low alpha to allow you to see densities of points
  facet_wrap(facets = ~updated_pitch_type) +
  labs(x = "Spin Rate (RPM)", y = "Break Angle (Degrees)", # need double check on degrees
       title = "Spin Rate and Break Angle for Each Pitch Type") + 
  theme(plot.title = element_text(hjust = 0.5))

```

The above figure has several interesting aspects. To start, every graph shows bounds along the upper and lower sides, with the same cone-shape. This is due to a sort of maximum angle break possible per a given spin rate. Across all pitches, this varies slightly, but every pitch type follows this similar cone shape, indicating a threshold for angle break given a spin rate. To note, the presence of the negative break angle axis comes from left and right-handed pitchers, changing how the ball flies through the air. 

Past simply spin rate and angle break, the shapes of each pitch type vary, slightly in some cases, and more significantly in other cases. For instance, a Knuckleball will not exceed a spin rate of 2200, based on this dataset. As such, knuckleballs also do not reach higher angle breaks. There are also noteworthy scenarios in the data where at higher spin rates, smaller angle breaks aren’t possible. This occurs for Changeups, Splitters, Sinkers, Two-seam Fastballs, Four-seam Fastballs, and other pitch types, to varying degrees. Really, the higher the spin rate, across most pitch types, the fewer pitches that have low angle breaks. The only exceptions are the Four-seam Fastball and the Cutter, which both have angle breaks of 0° at spin rates around 2500 rpm. This clearly shows different behaviors of the ball during flight for different pitch types, although there are more differences between some pitch types than others. In conclusion for this data visualization, there are distinct differences in angle break and spin rate mechanics based on the pitch type. To prove this, the next code chunk displays ANOVA tests of spin rate and break angle across pitch types, both of which are statistically significant. This indicates that both the spin rate and break angle of each pitch type is unique.

```{r anova tests and summary statistics}
# Means for each pitch type for spin rate and break rate
renamed_pitches_clean |>
  group_by(pitch_type) |>
  summarize(avg_break_angle = mean(break_angle),
            avg_spin_rate = mean(spin_rate)) ->
  avgs

head(avgs, 11)

# Anova tests
anova_spin <- aov(spin_rate ~ pitch_type, data = pitches)
anova_break <- aov(break_angle ~ pitch_type, data = pitches)
summary(anova_spin)
summary(anova_break)

```



One of the unique aspects of our dataset is the exact coordinates of each pitch as it crosses home plate. We were curious about pitcher strategy when it comes to the locations they aim for. When observing the location of all pitches from the 2018 season, the pitches tend to be most densely clustered around the center of the strike zone. When filtering for the pitchers throwing hand and the batter's stance, the data becomes much more indicative of pitcher strategy. The figures below show the normalized density of pitch location, where the lighter colors indicate a higher density of pitches at that location. 

```{r first heat map}
# saving locations for the average mlb strike zone
sz_width <- 17/12
sz_height <- mean(pitches$sz_top, na.rm = T) - mean(pitches$sz_bot, na.rm = T)
sz_offset <- mean(pitches$sz_bot, na.rm = T)

# finding sample size counts
sample_sizes <- rbind(
  c(nrow(filter(pitches, p_throws == "L" & stand == "L")), "L", "L"),
  c(nrow(filter(pitches, p_throws == "R" & stand == "L")), "R", "L"),
  c(nrow(filter(pitches, p_throws == "L" & stand == "R")), "L", "R"),
  c(nrow(filter(pitches, p_throws == "R" & stand == "R")), "R", "R")
) |> data.frame() |> rename("N" = X1,
                            "Pitcher" = X2,
                            "Batter" = X3)

pitches |> 
  select(stand, p_throws, pitch_type, code, px, pz) |> 
  filter(px >= -3 & px <= 3, # removing outlier pitch locations
         pz >= 0 & pz <= 6) |> 
  rename("Pitcher" = p_throws,
         "Batter" = stand) |> 
  ggplot(
    mapping = aes(
      x = px,
      y = pz
    )
  ) + 
  geom_density_2d_filled( # adding normalized density levels
    mapping = aes(fill = after_stat(level)),
    contour_var = "ndensity",
    adjust = .5,
    alpha = .8,
    color = "black",
    linewidth = .1,
    show.legend = F
  ) +
  geom_polygon( # drawing the strike zone
    data = data.frame(
      x = rep(c(-sz_width/2, sz_width/2), each=2),
      y = c(sz_offset + sz_height, sz_offset, sz_offset, sz_offset + sz_height)
    ),
    mapping = aes(x = x,
                  y = y),
    fill = NA,
    color = "black"
  ) + 
  facet_grid(
    rows = vars(Batter),
    cols = vars(Pitcher),
    labeller = label_both
  ) + 
  geom_text(
    data = sample_sizes,
    mapping = aes(label = paste("N:", N)),
    x = 1.6,
    y = 5,
    color = "white"
  ) + 
  coord_equal() + 
  labs(
    x = NULL,
    y = NULL,
    title = "Pitch Heatmap by Handedness",
    caption = "View is from behind home plate"
  ) +
  theme_bw() + 
  theme(plot.title = element_text(hjust = .5))



```


The data shows that all pitchers generally prefer to throw to the batters outside, no matter which side of the plate the batter lines up on. The pitcher favored matchups, where the pitchers throwing hand is the same as the batter’s stance, have noticeably different distributions compared to the batter favored matchups. In the pitcher favored matchups, the top of the zone has more pitches on the pitcher’s arm side, and at the lower areas of the plate, the pitches tail off to the glove side. 

We were also curious to see how pitch location is affected by pitch type. 


```{r second heat map}
pitches_labels <- c("4-Seam Fastball", "Slider", "2-Seam Fastball", "Changeup", "Curveball")
# creating dataframe for pitch type sample sizes
pitch_sample_sizes <-
  table(pitches$pitch_type) |> 
  data.frame() |> 
  filter(
    Var1 %in% c("FF", "SL", "FT", "CH", "CU", "SI")
  ) |> 
  rename("pitch_type" = Var1)



pitches |> 
  select(stand, p_throws, pitch_type, code, px, pz) |> 
  filter(px >= -3 & px <= 3, # removing outlier pitch locations
         pz >= 0 & pz <= 6,
         pitch_type %in% c("FF", "SL", "FT", "CH", "CU", "SI")) |> 
  ggplot(
    mapping = aes(
      x = px,
      y = pz
    )
  ) + 
  geom_density_2d_filled( # graphing normalized density
    mapping = aes(fill = after_stat(level)),
    contour_var = "ndensity",
    adjust = .5,
    alpha = .8,
    color = "black",
    linewidth = .1,
    show.legend = F
  ) +
  geom_polygon( # drawing strike zone
    data = data.frame(
      x = rep(c(-sz_width/2, sz_width/2), each=2),
      y = c(sz_offset + sz_height, sz_offset, sz_offset, sz_offset + sz_height)
    ),
    mapping = aes(x = x,
                  y = y),
    fill = NA,
    color = "black"
  ) + 
  facet_wrap(
    facets = ~pitch_type,
    labeller = labeller(pitch_type = c(FF = "4-Seam Fastball", SL = "Slider", FT = "2-Seam Fastball", CH = "Changeup", CU = "Curveball", SI = "Sinker"))
  ) + 
  geom_text(
    data = pitch_sample_sizes,
    mapping = aes(label = paste("N:", Freq)),
    x = 1.5,
    y = 5,
    color = "white",
    size = 3
  ) + 
  coord_equal() + 
  labs(title = "Location Heat Map By Pitch",
       caption = "With average mlb strike zone \nX,Y in feet ",
       x = NULL,
       y = NULL) +
  theme(title = element_text(hjust =.5))



```

The distributions show 4 - seam fastballs as the pitch that generally occupies the top half of the strike zone, and sliders, curveballs, and changeups occupy the lower parts of the zone. The location of sliders is very right biased, which makes sense based on the nature of the pitch. Sliders in the modern MLB usually have a lot of glove-side movement, and many more pitches are thrown by right-handers, which explains why the slider tends to enter the zone on the right half of the plate. I predict that when splitting the data between left-handed and right-handed pitchers, the higher density areas for sliders would be on opposite sides of home plate. 


```{r Elite vs Non-Elite Pitcher Comps}
elite_pitchers <- 
  pitches |>
  filter(pitcher_id == 425794 | pitcher_id == 433587 | pitcher_id == 446372 | pitcher_id == 456034 | pitcher_id == 453286 | pitcher_id == 434378 | pitcher_id == 519144 | pitcher_id == 502042 | pitcher_id == 425844 | pitcher_id == 594798 | pitcher_id == 543606 | pitcher_id == 543037 | pitcher_id == 545333 | pitcher_id == 453562 | pitcher_id == 605400 | pitcher_id == 518516 | pitcher_id == 457918 | pitcher_id == 461829 | pitcher_id == 519242 | pitcher_id == 500779 | pitcher_id == 605483 | pitcher_id == 452657 | pitcher_id == 430935 | pitcher_id == 572971 | pitcher_id == 571578)

# Finding average for key pitching variables
elite_avg <- 
  elite_pitchers |>
  summarise(avgvelo = mean(start_speed, na.rm = TRUE),
            avgbreak = mean(break_length, na.rm = TRUE),
            avgspin = mean(spin_rate, na.rm = TRUE)
            )

elite_pitchers <- elite_pitchers %>%
  mutate(is_strikeout = ifelse(event %in% c('Strikeout', 'Strikeout - DP'), 1, 0), is_out = ifelse(event %in% c('Flyout', 'Groundout', 'Forceout', 'Double Play', 'Fielders Choice Out', 'Lineout', 'Pop Out', 'Runner Out'), 1, 0))

non_elite <- pitches %>% 
filter(!pitcher_id %in% c(425794, 433587, 446372, 456034, 453286, 434378, 519144, 502042, 425844, 594798, 543606, 543037, 545333, 453562, 605400, 518516, 457918, 461829, 519242, 500779, 605483, 452657, 430935, 572971, 571578))

nonelite_avg <- 
  non_elite |>
  summarise(avgvelo = mean(start_speed, na.rm = TRUE),
            avgbreak = mean(break_length, na.rm = TRUE),
            avgspin = mean(spin_rate, na.rm = TRUE))

elite_event_counts <- elite_pitchers |>
  group_by(event) |>
  summarise(count = n()) |>
  arrange(count) 

elite_event_counts$proportion <- elite_event_counts$count / sum(elite_event_counts$count)

elite_event_counts1 <- elite_event_counts[-(1:19), ] # Removing rarer events

nonelite_event_counts <- non_elite |>
  group_by(event) |>
  summarise(count = n()) |>
  arrange(count) 

nonelite_event_counts$proportion <- nonelite_event_counts$count / sum(nonelite_event_counts$count)
nonelite_event_counts1 <- nonelite_event_counts[-(1:21), ] # Non elite data includes two outcomes not present in elite set

# Add a new column to each dataset to indicate the group
elite_event_counts1$group <- 'Elite'
nonelite_event_counts1$group <- 'Non-Elite'

# Use full_join to combine the datasets by event type
combined_events <- full_join(elite_event_counts1, nonelite_event_counts1, by = "event", suffix = c("_elite", "_nonelite"))

# Calculate the difference in proportions
combined_events <- combined_events |>
  mutate(diff = proportion_elite - proportion_nonelite)

# Plot the differential heatmap
ggplot(combined_events, aes(x = event, y = 1, fill = diff)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "red", high = "steelblue", mid = "white", midpoint = 0, limit = c(-0.02, 0.05)) +
  labs(title = 'Difference in Proportion of AB outcomes (Elite - Non-Elite)', x = "Event Type", y = "", fill = "Difference") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())

# Filter for productive events
productive_events <- c('Intent Walk', 'Triple', 'Hit By Pitch', 'Sac Bunt', 'Sac Fly', 'Home Run', 'Double', 'Walk', 'Single')
elite_productive <- elite_event_counts %>%
  filter(event %in% productive_events)
nonelite_productive <- nonelite_event_counts %>%
  filter(event %in% productive_events)

# Combine the elite and non-elite dataframes
productive_AB <- bind_rows(
  mutate(elite_productive, group = 'Elite'),
  mutate(nonelite_productive, group = 'Non-Elite')
)

# Plotting the side-by-side bar chart
ggplot(productive_AB, aes(x = event, y = proportion, fill = group)) +
  geom_bar(stat = 'identity', position = 'dodge', color = 'black', linewidth = 0.5) +
  scale_fill_manual(values = c('Elite' = 'blue', 'Non-Elite' = 'red')) +
  labs(title = 'Proportion of Productive At-Bats: Elite vs Non-Elite Pitchers', x = 'Event Type', y = 'Proportion', fill = 'Group') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# The most statistically significant is in the walk rate, which further proves the 'Moneyball' theory that OBP% is one of the most important stats and elite pitchers limit free passes.

```

We wanted to visualize how batters fared against the 25 ‘elite’ pitchers selected (15 RHP and 10 LHP), who were selected based on 2018 pitching leaderboards and all-star teams. We removed the 13 rarest outcomes for an at-bat, including catcher’s interference, strikeout double play, runner out, among others. However, the pie chart still includes pieces that are barely visible, like triple, sac fly, field error, and hit by pitch which we may end up removing to make it easier for the viewer to interpret. Additionally, we filtered out only the events that are deemed ‘productive’ for the batter, where either they reach base themselves or sacrifice to advance a runner. We can see that elite pitchers limit extra base hits very well, over ¾ of the productive results for a batter is either a walk or a single. This is unsurprising, as teams rely on their aces to pitch lots of innings and they don’t want to be beat on just a single pitch. 


```{r Zack Greinke Case Study: Velocity Analysis and Heat Map}
greinke <-
  elite_pitchers |>
    filter(pitcher_id == 425844) |>
    select(px, pz, start_speed, end_speed, spin_rate, spin_dir, break_angle, break_length, ax, ay, az, nasty, zone, code, type, pitch_type, b_count, s_count, outs, pitch_num, on_1b, on_2b, on_3b, event, inning, o, stand, is_strikeout, is_out) 

# Analyzing Greinke's distribution of pitch type
greinke_pitchtype <- greinke |>
  group_by(pitch_type) |>
  summarise(count = n()) 

greinke_pitchtype_dist <- greinke_pitchtype %>%
  mutate(pitch_type = case_when(
    pitch_type == "CH" ~ "Changeup",
    pitch_type == "CU" ~ "Curveball",
    pitch_type == "EP" ~ "Eephus",
    pitch_type == "FF" ~ "4-seam fastball",
    pitch_type == "SL" ~ "Slider",
    pitch_type == "FT" ~ "2-seam fastball",
    FALSE ~ as.character(pitch_type)  # Handles any other pitch types not listed
  ))

# Calculate the total number of pitches
total_pitches <- sum(greinke_pitchtype$count)

# Calculate the proportion for each pitch type
greinke_pitchtype_dist |>
  mutate(proportion = count / total_pitches) |>
  arrange(desc(count))

# Analyzing Greinke's avg velocity and velocity from early to late innings
greinke_velo <- greinke |>
  group_by(pitch_type) |>
  summarise(average_velocity = mean(start_speed, na.rm = FALSE))

greinke_velo_inning <- greinke |>
  group_by(inning, pitch_type) |>
  summarise(average_velocity = mean(start_speed, na.rm = TRUE)) |>
  ungroup()  # Ungroup for plotting

# Plotting the line chart 
ggplot(greinke_velo_inning, aes(x = inning, y = average_velocity, color = pitch_type, group = pitch_type)) +
  geom_line() +
  scale_color_manual(values = c("CH" = "red", "CU" = "green", "FF" = "blue", "FT" = "purple", "EP" = "orange", "SL" = "pink"),
                     labels = c("CH" = "Changeup", "CU" = "Curveball", "FF" = "4-seam Fastball", "FT" = "2-seam Fastball", "EP" = "Eephus", "SL" = "Slider")) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),
    panel.grid.major = element_blank(),
    axis.text.x = element_text(hjust = 1),
    legend.title = element_blank()
  ) +
  labs(title = "Zack Greinke: Average Pitch Velocity by Inning", x = 'Inning', y = 'Velocity') +
  scale_y_continuous(breaks = seq(from = 63,
                                  to = 93,
                                  by = 5)) +
  scale_x_continuous(breaks = 1:max(greinke$inning))  

greinke_pitchlabels <- c("4-Seam Fastball", "Slider", "2-Seam Fastball", "Changeup", "Curveball", 'Eephus')
# creating dataframe for pitch type sample sizes
greinke_sample_sizes <-
  table(greinke$pitch_type) |> 
  data.frame() |> 
  filter(
    Var1 %in% c("FF", "SL", "FT", "CH", "CU", "EP")
  ) |> 
  rename("pitch_type" = Var1)

greinke |> 
  select(pitch_type, code, px, pz) |> 
  filter(px >= -3 & px <= 3, # removing outlier pitch locations
         pz >= 0 & pz <= 6,
         pitch_type %in% c("FF", "SL", "FT", "CH", "CU", "EP")) |> 
  ggplot( mapping = aes(
      x = px,
      y = pz
    )
  ) + 
  geom_density_2d_filled( 
    mapping = aes(fill = after_stat(level)),
    contour_var = "ndensity",
    adjust = .5,
    alpha = .8,
    color = "black",
    linewidth = .1,
    show.legend = F
  ) +
  geom_polygon( # drawing strike zone
    data = data.frame(
      x = rep(c(-sz_width/2, sz_width/2), each=2),
      y = c(sz_offset + sz_height, sz_offset, sz_offset, sz_offset + sz_height)
    ),
    mapping = aes(x = x,
                  y = y),
    fill = NA,
    color = "black"
  ) + 
  facet_wrap(
    facets = ~pitch_type,
    labeller = labeller(pitch_type = c(FF = "4-Seam Fastball", SL = "Slider", FT = "2-Seam Fastball", CH = "Changeup", CU = "Curveball", SI = "Sinker", EP = 'Eephus' ))  ) + 
  geom_text(
    data = greinke_sample_sizes,
    mapping = aes(label = paste("N:", Freq)),
    x = 1.5,
    y = 4,
    color = "white",
    size = 3
  ) + 
  coord_equal() + 
  labs(title = "Zack Greinke: Location Heat Map By Pitch Type",
       caption = "With average mlb strike zone \nX,Y in feet ",
       x = NULL,
       y = NULL) +
  theme(title = element_text(hjust =.5))

greinke_pitchtype_dist
greinke_velo


```


## III. Machine Learning Methods

We developed a classification tree to determine the pitch type. To do this, we first removed variables from the data that had to do with the game state. This was because the high number of variables and large amount of data made it take lengthy amounts of time. As such, we decided to reduce the number of columns. First, we removed all Id and Name columns from the data, as these were specific to games, pitchers, and batters. The presence of these as categorical variables of many many types would have made calculation of a classification tree both large and very specific to the batters and pitchers in the dataset. Additionally, things like the game id were simply unnecessary. The following code chunk shows the data cleaning necessary to remove all the columns. Then, we decided to shift the goal of this classification tree to determine the pitch type solely based on data having to do with the sensor data, such as positions, velocities, spin rates, etc. This eliminated even more variables, leaving 25 variables having to do with pitch sensor data.

```{r additional cleaning}
# Select pitch types and change to factor types
pitches_filtered <- pitches |>
  select(-c(pitcher_name, batter_name, pitcher_id, g_id, batter_id, ab_id)) |>
  filter(!pitch_type %in% c("", "AB","EP","PO", "FO")) |>
  mutate(code = as.factor(code),
         type = as.factor(code),
         zone = as.factor(zone),
         pitch_type = as.factor(pitch_type),
         across(where(is.character), as.factor))

# determine which columns to remove
pitches_filtered |>
  select(where(is.factor)) |>
  colnames() ->
  to_remove

to_remove <- to_remove[to_remove != "pitch_type"]

pitches_filtered2 <- pitches_filtered |>
  # The following select removes all unnecessary variables (unnecessary having to do with anything but the pitch itself)
  select(-all_of(to_remove), # This keeps pitch_type, which is needed (Rstudio recommended all_of)
         -c(inning, p_score, o, outs, on_1b, on_2b, on_3b, outs, 
            event_num, outs, b_score, b_count, s_count, type_confidence, pitch_num))

```


After this was performed, we constructed a classification tree using Rpart. Then, we pruned the tree, determined variables of importance, and calculated the accuracy of the tree. The next code chunk contains all of this and the variable importance graph below shows the variables of importance. Unfortunately, the tree does have 4569 leaf nodes, which makes the resulting graph far too large to graph. While this seems very large for a classification tree, the unpruned version had 68571 leaf nodes, making the pruned version a fraction of the size.

```{r full classification tree and pruning}

# Build the full decision tree here (This will take a while to do, mine took around 15 minutes)
pitch_full_tree <- rpart(
  formula = pitch_type ~ .,   # explanatory and response variables
  data = pitches_filtered2,
  method = "class",
  parms = list(split = "information"), # Using entropy
  minsplit = 0,
  minbucket = 0,
  cp = -1
)


pitch_full_tree$cptable |> 
  data.frame() |>
  # finding row with smallest xerror
  slice_min(xerror, n=1, with_ties = F) |>
  # create xerror cutoff = xerror + xstd
  mutate(xerror_cutoff = xerror + xstd) |>
  # picking xerror_cutoff table
  pull(xerror_cutoff) ->   # Saves as vector, only give one column
  xcutoff

# Finding the "best" tree using xcutoff
pitch_full_tree$cptable |> 
  data.frame() |>
  # Keeping all rows below xcutoff
  filter(xerror < xcutoff) |>      
  # Get simplest one
  slice(1)

# Finding the "best" tree using xcutoff
pitch_full_tree$cptable |> 
  data.frame() |>
  # Keeping all rows below xcutoff
  filter(xerror < xcutoff) |>  
  # Get simplest one
  slice(1) |>
  # Picking cp value out of dataframe 
  pull(CP) ->
  cp_prune

c("xerror cutoff" = xcutoff,
  "cp prune value" = cp_prune)

pitches_pruned <- prune(tree = pitch_full_tree,
                     cp = cp_prune)

```

```{r variables of importance}

# This code is taken directly from class, plot importance of variables
caret::varImp(pitches_pruned) |> 
  arrange(desc(Overall)) |> 
  rownames_to_column(var = "variable") |> 
  
  ggplot(mapping = aes(x = fct_reorder(variable, -Overall),
                       y = Overall)) + 
  
  geom_col(fill = "steelblue",
           color = "black") + 
  
  labs(x = NULL,
       y = "Variable Importance",
       title = "Variable Importance in Pruned Classification Tree for Pitch type data") + 
  
  scale_y_continuous(expand = c(0, 0, 0.05, 0)) +
  
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```

```{r prediction and accuracies}

predict(
  object = pitches_pruned,
  newdata = pitches_filtered2,
  type = "class"
) ->
  pitches_predicted

# Creating the confusion matrix:
confusionMatrix(
  data = pitches_predicted,
  reference = pitches_filtered2$pitch_type
)

```

Based on these figures, especially the variable importance figure, we decided to try moving forward with reducing the complexity of the classification tree by removing unimportant variables. This entailed removing the variables; strikezone top and bottom, break in the y direction, and starting y position. This increased the number of splits in the classification tree from 4568 to 6886, with an overall accuracy increase from 86.46% to 87.59%. This was done using the same pipeline as outlined within the previous three code chunks, with the only difference being removing the 4 least important variables from the data. The drastic increase in complexity accompanying this change, while hardly increasing accuracy was determined not to be useful. As such, the final classification tree for determining pitch type uses the variables shown in the variable importance graph. The importance of these variables is very interesting to see, with break length, start speed, velocity in the y direction, acceleration in the z direction, and end speed are the five most important variables. Close after this is spin rate, which makes sense as a useful variable for classification based on our prior data analysis. Break angle, another variable we had analyzed previously, was less useful, but useful nonetheless. 



We then developed a KNN model to determine the result of a given pitch, once again based on sensor data. The possible outcomes included a ball, ball in the dirt, swinging strike, called strike, foul, foul tip, intentional ball, and more. The pipeline for this followed a similar path to before of removing categorical data, except for code and pitch type, and then filtering for a single pitch type, Knuckle Curve This was because running a KNN model on the entire dataset would have taken far too long. Determining the best choice of K with this in mind would've taken much longer. Because of this, we determined that the Knuckle Curve was a good pitch type to try to predict the outcome of since there were 16327 data entries. This is a pretty decent amount of data, but considering there are 13 levels, with varying levels in each group, the Knuckle curve did a good job of providing enough data points for KNN to work.

```{r pitch types}
# Show the amounts of each pitch type
table(pitches$pitch_type)

```

We proceed with data cleaning as discussed.

```{r clean for knn}

# Need to remove batter name and pitcher name, and all ids 
# also need to filter out bad pitch data
pitches_filtered_knn <- pitches |>
  select(-c(pitcher_name, batter_name, pitcher_id, g_id, batter_id, ab_id)) |>
  filter(!pitch_type %in% c("", "AB","EP","PO", "FO")) |>
  mutate(code = as.factor(code),
         type = as.factor(code),
         zone = as.factor(zone),
         pitch_type = as.factor(pitch_type),
         across(where(is.character), as.factor)) |>
  filter(pitch_type == "KC") 


# Remove factor columns (can't have these in KNN)
pitches_filtered_knn |>
  select(where(is.factor)) |>
  colnames() ->
  to_remove

to_remove <- to_remove[to_remove != "code"]
  # The following select removes all unnecessary variables (unnecessary having to do with anything but the pitch itself)
pitches_filtered_knn2 <- pitches_filtered_knn |>
  select(-all_of(to_remove), # This keeps pitch_type, which is needed (Rstudio recommended all_of)
         -c(inning, p_score, o, outs, on_1b, on_2b, on_3b, outs, 
            event_num, outs, b_score, b_count, s_count, type_confidence, pitch_num))

# Make sure correct columns are removed
tibble(head(pitches_filtered_knn2))
# unique(pitches_filtered_knn2$code) # Use this to check the codes

```


Since KNN needs normalized or standardized data, we need to standardize and normalize the data.

```{r normalize and standardize data}
# normalize function
normalize <- 
  function(x){
  return((x - min(x))/(max(x) - min(x)))
}

# Normalizing the data:
knn_norm <- 
  pitches_filtered_knn2 |> 
  mutate(across(.cols = where(is.numeric),
                .fns = normalize))

# Checking that the min is 0 and max is 1 for both columns:
knn_norm |> 
  summarize(across(.cols = where(is.numeric),
                   .fns = list(min = min, 
                               max = max)))

# standardize function
standardize <- 
  function(x){
    return((x - mean(x))/sd(x))
  }

# standardizing the data:
knn_stan <- 
  pitches_filtered_knn2 |> 
  mutate(across(.cols = where(is.numeric),
                .fns = standardize))

# Checking that the mean is 0 and the standard deviation is 1 for both columns:
knn_stan |> 
  summarize(across(.cols = where(is.numeric),
                   .fns = list(avg = mean, 
                               sd = sd))) |> 
  round(digits = 5)

```

Considering there is still a large amount of data, we will be using partitioning. LOOCV/the holdout method would also work, but would probably take longer to run. Additionally, we randomly sampled from the dataset in order to not introduce any form of bias towards the earlier season or later season in 2018.

```{r train test split}
RNGversion("4.1.0")
set.seed(2870)
# Perform train test split to separate the data
# From https://www.statology.org/train-test-split-r/
sample <- sample(c(TRUE, FALSE), nrow(pitches_filtered_knn), replace=TRUE, prob=c(0.8,0.2))
train_stan <- knn_stan[sample, ]
test_stan <- knn_stan[!sample, ]
train_norm <- knn_norm[sample, ]
test_norm <- knn_norm[!sample, ]


```


Next, we perform a K search, looking for the best accuracy across the standardized and normalized data for several selections of k. We unfortunately cannot test every single possible selection of k, since this would be computationally expensive and we do not have such resources at our disposal. Because of this, a range of ks has been chosen that outlines the maximum accuracy found for k, through the searching of ks from 5 to 500. These won't all be included in the RMD, as a result of the time this would take to do.

```{r knn choosing k}
RNGversion("4.1.0")
set.seed(2870)

# It turns out that using all columns for training isn't too long in run time
knn_cols <- train_norm |> select(where(is.numeric)) |> colnames()

# Data frame for knn accuracy - if the chosen value is 100, may need to increase upper bound
knn_acc <- 
  tibble(
    k = 10:100, # this will take a while but is necessary for the graph below
    norm_acc = rep(-1, length(k)),
    stan_acc = rep(-1, length(k))
  ) 

for (i in 1:nrow(knn_acc)) {
  # Use normalized data
  # Fitting knn algorithm for ith iteration of loop
  loop_norm <- knn(train = train_norm[, knn_cols],
                  test = test_norm[, knn_cols],
                  cl = train_norm$code,
                  k = knn_acc$k[i])
  
  # forming confusion matrix for ith iteration of normalized data
  loop_cm_norm <- confusionMatrix(
    data = loop_norm, 
    reference = test_norm$code
  )
  
  # store accuracy
  knn_acc[i, 2] <- loop_cm_norm$overall[1]
  
  # Use standardized data
  # Fitting knn algorithm for ith iteration of loop
  loop_stan <- knn(train = train_stan[, knn_cols],
                  test = test_stan[, knn_cols],
                  cl = train_stan$code,
                  k = knn_acc$k[i])
  
  # forming confusion matrix for ith iteration of normalized data
  loop_cm_stan <- confusionMatrix(
    data = loop_stan, 
    reference = test_stan$code
  )
  
  # same as confusion matrix
  knn_acc[i, 3] <- loop_cm_stan$overall[1]
}

knn_acc

knn_acc |>
  pivot_longer(cols = -k,
               names_to = "rescale_method",
               values_to = "accuracy") |> 
  slice_max(accuracy)

```

Finally, below is a graph of K with corresponding accuracies. As we can see, for predicting the outcome of pitches in the 2018 MLB data set, 47.71% of pitches are correctly identified using KNN on sensor data. While this accuracy does seem low, there are factors to take into account that we couldn't do so in our data. Each pitcher and batter are different, and the state of the game may have influence on the performance on either of them. As such, this approach of using just sensor data may no be the greatest approach, but KNN also cannot use the categorical data necessary to describe the game state. Because of this, KNN is not the optimal method for determining outcome, but it is still better than nothing, as given by the following code chunk.

```{r best accuracy}
# Checking the results using resubstitution
predicted_norm <- knn(train = train_stan[, knn_cols],
                  test = test_stan[, knn_cols],
                  cl = train_stan$code,
                  k = 57)

confusionMatrix(
    data = predicted_norm, 
    reference = test_stan$code
  )


```

Even though this is a poor overall accuracy, it is still statistically significant over the no information rate. It is also noteworthy that Classes D, E, H, L, M, and T all have no predictions. This is because these are present in low quantities in the data, so KNN struggles to predict those codes. This comes from the fact that the data is all very interspersed with different classes, making it harder to predict smaller quantity classes. The following code chunk displays this. Even though this would be nice to be avoided, increasing our data set to the next highest pitch type, FC (Cutter), more than doubles the size of the data that is being predicted on. This is a large problem because of run time, as searching for k in the above data already takes upwards of 20 minutes.

```{r see codes}
# View the codes of KC
table(pitches_filtered_knn$code)
```


This is also taking into account the best possible choice of K, being 57. This means that to determine the outcome of a single point, with 47.71% accuracy, we can determine the outcome of a pitch. This being the case, we can see the accuracies for given ks from 10 to 100 in the following graph.

```{r accuracies per k}
# Plot k
knn_acc |> 
  pivot_longer(cols = -k,
               names_to = "rescale_method",
               values_to = "accuracy") |> 
  ggplot(mapping = aes(x=k, y=accuracy)) +
  geom_line(mapping = aes(color = rescale_method)) +
  labs(title = "KNN accuracy over choice Ks",
       x = "Choice of k",
       y = "Accuracy",
       color = "Rescale Method") +
  scale_color_manual(values = c("norm_acc" = "darkorange", "stan_acc" = "steelblue"),
                     labels = c("norm_acc" = "Normalize", "stan_acc" = "Standardized")) +
  theme(plot.title = element_text(hjust = 0.5))
  
```


This graph is very interesting since standardized accuracy is much higher than normalized accuracy. It also looks like the accuracy begins to trend upwards with increasing k. This is not the case, as given in the following graph.

```{r knn showing max k}
RNGversion("4.1.0")
set.seed(2870)

# It turns out that using all columns for training isn't too long in run time
knn_cols <- train_norm |> select(where(is.numeric)) |> colnames()

# Data frame for knn accuracy - if the chosen value is 100, may need to increase upper bound
knn_acc <- 
  tibble(
    k = seq(from = 11, to = 411, by = 20), # this will take a while but is necessary for the graph below
    norm_acc = rep(-1, length(k)),
    stan_acc = rep(-1, length(k))
  ) 

for (i in 1:nrow(knn_acc)) {
  # Use normalized data
  # Fitting knn algorithm for ith iteration of loop
  loop_norm <- knn(train = train_norm[, knn_cols],
                  test = test_norm[, knn_cols],
                  cl = train_norm$code,
                  k = knn_acc$k[i])
  
  # forming confusion matrix for ith iteration of normalized data
  loop_cm_norm <- confusionMatrix(
    data = loop_norm, 
    reference = test_norm$code
  )
  
  # store accuracy
  knn_acc[i, 2] <- loop_cm_norm$overall[1]
  
  # Use standardized data
  # Fitting knn algorithm for ith iteration of loop
  loop_stan <- knn(train = train_stan[, knn_cols],
                  test = test_stan[, knn_cols],
                  cl = train_stan$code,
                  k = knn_acc$k[i])
  
  # forming confusion matrix for ith iteration of normalized data
  loop_cm_stan <- confusionMatrix(
    data = loop_stan, 
    reference = test_stan$code
  )
  
  # same as confusion matrix
  knn_acc[i, 3] <- loop_cm_stan$overall[1]
}

# Plot k
knn_acc |> 
  pivot_longer(cols = -k,
               names_to = "rescale_method",
               values_to = "accuracy") |> 
  ggplot(mapping = aes(x=k, y=accuracy)) +
  geom_line(mapping = aes(color = rescale_method)) +
  labs(title = "KNN accuracy over choice Ks",
       x = "Choice of k",
       y = "Accuracy",
       color = "Rescale Method") +
  scale_color_manual(values = c("norm_acc" = "darkorange", "stan_acc" = "steelblue"),
                     labels = c("norm_acc" = "Normalize", "stan_acc" = "Standardized")) +
  theme(plot.title = element_text(hjust = 0.5))

```

This clearly indicates that we have our maximum choice of k. Increasing k even further past this point quickly runs into run time errors. With the downward trend of accuracy with increasing k with the range in the above graph, we determined that this was sufficient evidence for our choice of k


We found our data from a collection of datasets posted on kaggle.com, and in that collection there was a pitches dataset from 2019. We ideally would have liked to use the most recent data for our exploration, but the key setback for this dataset was that it was missing all of the entries for spin rate. Among MLB pitcher analysis, spin rate is a very important stat, that also has controversy surrounding it. Making the baseball spin is what gives a pitch its movement, making it more difficult for the batter to track. For years, MLB pitchers have been trying to find ways to increase the RPM on their pitches, most notebly by using illegal foreign substances on the mound to get a better grip on the ball. With so much emphasis on spin rate in the MLB, we set out to find a way to predict spin rate based on the other sensor information we have on the pitch. To do this, we created a regression tree model, because after analysis we found no real strong linear relationships between spinrate and our explanatory variables, and we would like to use an eager learning method so that we can apply the model to the 2019 data. The key challenge with creating a regression tree with the size of our data would be the training time it would take. So, to adjust for this, we decided to focus on one pitcher from the data, the 2018 National League Cy Young award winner Jacob deGrom. 

```{r spin rate eda}
# filtering to only pitches thrown by deGrom
degrom <- pitches |> 
  filter(pitcher_name == "Jacob deGrom",
         !is.na(spin_rate))


# deciding what factors are completely unrelated to the pitch
degrom |> 
  select(
    -sz_top, -sz_bot, -type_confidence, -y0
  ) -> degrom

# Keeping data about only the pitch, not the at bat outcome
degrom |> 
  select(start_speed:pitch_type, -zone, -break_y) |> 
  mutate(across(.cols = where(is.character), .fns = factor)) -> degrom

# EDA to see relationships between numeric variables
degrom |> 
  relocate(spin_rate) |> 
  pivot_longer(
    cols = start_speed:nasty,
    names_to = "stat",
    values_to = "measurement"
  ) |> 
  ggplot(
    mapping = aes(x = measurement, y = spin_rate)
  ) + 
  geom_point(alpha = .2) + 
  geom_smooth(
    method = 'loess',
    se = F,
    formula = y~x
  ) + 
  facet_wrap(
    facets = ~stat,
    scales = "free_x"
  ) + 
  labs(
    title = "Spin rate relationship with numeric sensor data",
    y = "Spin Rate (RPM)",
    x = NULL
  )

```
In our graphs, we see in many of the measurements clusters of points forming, likely due to the pitch type. Each pitch type has a different direction of spin, so naturally they tend to cluster around different spin rates. Using a regression tree will allow us to make splits in the data based on pitch type so that they are treated differently in the prediction. 

```{r spin rate regression}
# making the fully formed tree
spin_rate_full <- 
  rpart(
    formula = spin_rate ~ .,
    data = degrom,
    method = "anova",
    minsplit = 2,
    minbucket = 1,
    cp = -1
  )


# Finding the xerror cut off
spin_rate_full$cptable |> 
  data.frame() |> 
  slice_min(xerror, n = 1, with_ties = F) |> 
  mutate(xcutoff = xerror + xstd) |> 
  pull(xcutoff) -> xcutoff




# Find the cp value to prune the tree at
spin_rate_full$cptable |> 
  data.frame() |> 
  # finding trees with xerror below xcutoff
  filter(xerror <= xcutoff) |> 
  slice(1) |> 
  pull(CP) -> cp_prune

# pruning the tree
prune(
  spin_rate_full,
  cp = cp_prune
) -> spin_rate_pruned


# making predictions
pred_sr <-
  predict(object = spin_rate_pruned, 
          newdata = degrom)
# merging predicted spin rates with training data
sr_pred <- 
  degrom |> 
  mutate(pred_sr = pred_sr)
# calculating R2 and rmse
sr_pred |> 
  summarize(
    R2 = cor(spin_rate, pred_sr)^2,
    rmse = sqrt(sum((spin_rate - pred_sr)^2)/(n() - 1))
  )

## plotting prediction residuals

ggplot(
  data = sr_pred,
  mapping = aes(
    x = pred_sr,
    y = (spin_rate - pred_sr)
  )
) + 
  geom_point() + 
  geom_hline(
    yintercept = 0,
    color = "red",
    linetype = "dashed",
    linewidth = 1
  ) + 
  labs(
    x = "Predicted Spin Rate",
    y = "Residual"
  )

```

As you can see from the graph, R^2 and rmse values, the regression tree fits very well for deGrom. The tree makes predicts spin rates that cluster very tightly around the line y=x, so there is little error when dealing with pitches thrown by deGrom. But, as I mentioned earlier, the 2019 data is misssing spin rate measurements for all pitches thrown, and we don't know if this model trained from deGrom's pitches will fit nearly as well with data from other pitchers. The code chunk below will test this model on pitches thrown by another elite pitcher, 2018 AL Cy Young winner Blake Snell, as well as a random subset from the data. 

```{r spin rate comparison snell}
# creating dataset for Blake Snell
snell <- pitches |> 
  filter(pitcher_name == "Blake Snell",
         !is.na(spin_rate))

snell |> 
  select(
    -sz_top, -sz_bot, -type_confidence, -y0
  ) -> snell
snell |> 
  select(start_speed:pitch_type) |> 
  mutate(across(.cols = where(is.character), .fns = factor)) -> snell


# making predictions
snell_pred_sr <-
  predict(object = spin_rate_pruned, 
          newdata = snell)
# merging predicted spin rates with training data
snell_sr_pred <- 
  snell |> 
  mutate(pred_sr = snell_pred_sr)
# calculating R2 and rmse
snell_sr_pred |> 
  summarize(
    R2 = cor(spin_rate, pred_sr)^2,
    rmse = sqrt(sum((spin_rate - pred_sr)^2)/(n() - 1))
  )

## plotting predictions

ggplot(
  data = snell_sr_pred,
  mapping = aes(
    x = pred_sr,
    y = (spin_rate - pred_sr)
  )
) + 
  geom_point() + 
  geom_hline(
    yintercept = 0,
    color = "red",
    linetype = "dashed",
    linewidth = 1
  ) + 
  labs(
    x = "Predicted Spin Rate",
    y = "Residual"
  )

```
```{r random spin rate comparison}



# creating a random subset
random_pitchers <- pitches |> 
  slice_sample(n = nrow(degrom)) |> 
  filter(code %in% unique(degrom$code),
         pitch_type %in% unique(degrom$pitch_type))

random_pitchers |> 
  select(
    -sz_top, -sz_bot, -type_confidence, -y0
  ) -> random_pitchers
random_pitchers |> 
  select(start_speed:pitch_type) |> 
  mutate(across(.cols = where(is.character), .fns = factor)) -> random_pitchers


# making predictions
random_pred_sr <-
  predict(object = spin_rate_pruned, 
          newdata = random_pitchers)
# merging predicted spin rates with training data
random_sr_pred <- 
  random_pitchers |> 
  mutate(pred_sr = random_pred_sr)
# calculating R2 and rmse
random_sr_pred |> 
  summarize(
    R2 = cor(spin_rate, pred_sr)^2,
    rmse = sqrt(sum((spin_rate - pred_sr)^2)/(n() - 1))
  )

## plotting predictions

ggplot(
  data = random_sr_pred,
  mapping = aes(
    x = pred_sr,
    y = (spin_rate - pred_sr)
  )
) + 
  geom_point() + 
  geom_hline(
    yintercept = 0,
    color = "red",
    linetype = "dashed",
    linewidth = 1
  ) + 
  labs(
    x = "Predicted Spin Rate",
    y = "Residual"
  )


```
As you can see from the outputs for both Blake Snell and a random subset of the data, the model based on deGrom's pitches is much worse when applied to the rest of the MLB, but the predictions still don't differ by a great amount. The deGrom model tends to under predict spin rate, which is the opposite of what I would have expected considering the success deGrom had on the mound in 2018. It is clear that our model is biased, and making a more complex model trained with a larger portion of the data may lead to a model that does better for other pitchers. 


## IV. Conclusions

In this study, we analyzed many variables having to do with pitches in the 2018 MLB pitching dataset. To start, we determined significant differences in spin rate and break rate based on the type of the pitch. We found that all pitches share some characteristics, such as maximum possible break angles per spin rate, but some pitches vary in their other characteristics. For instance, some pitches don’t reach higher spin rates or some pitches don’t have lower break angles at high spin rates. This provided enough information to pursue a classification tree based on spin rate, break angle, and many other possible variables.

One particular aspect we were interested in was where the ‘elite’ pitchers stand out compared to the rest of their peers. We manually selected 25 starting pitchers, 15 RHP and 10 LHP, based on the 2018 league leaders & all-stars. One thing done to compare elite pitchers to everyone else was filtering pitch type to only the 3 kinds of fastballs (4-seam, 2-seam, and cutter) and calculating average velocity compared to the rest of the dataset, which unsurprisingly ended up higher. 

The conversation around what makes a great MLB pitcher often revolves around the concept of “command”. Having good command over a pitch essentially means that it goes where the pitcher wants it to. In the analysis of pitch locations for all the pitches in the 2018 season, it starts to reveal a general league strategy based on a couple of factors that we analyzed, namely pitch type and dominant hand. These insights allow for a more exact criterion for what it takes for a pitcher to have command over their pitches. Pitchers in the MLB are usually judged by their outcome stats, like the amount of hits or runs they give up, which are dependent on many other factors of the game. But pitch location is almost completely up to the pitcher. So if a pitcher can keep their fastballs in the upper half of the strike zone, and keep sliders away from the middle of the plate, it can give a more quantitative view of their command. 

Our classification tree was able to determine the type of pitch based on sensor data with 86.46% accuracy. While this tree is arguably complex, having 4568 leaf nodes, there are also over 700,000 data entries and 25 variables being used for classification. This classification tree was also much better at predicting pitch types than simply choosing the most common pitch type, whose accuracy was 35.39%. The resulting variables of importance within the classification tree also were interesting, with break length, start speed, velocity in the y direction, acceleration in the z direction, and end speed being the five most important variables. The variable importance graph outlines this very well.

Our KNN model was able to determine the outcome of a pitch with 47.71% accuracy. This was with a k of 57, which while arguably complex, this was for a data set of 16327 data entries, which makes 57 a very low choice for k. We additionally only tested our KNN model over around 400 choices of k, due to time constraints and analysis finding a general downwards trend past a k of 100. There were additionally errors present having to do with ties when increasing k past 400. The likely reason for this low accuracy was because of the data used within KNN. This was purely sensor data about the pitch, which is not all that determines the outcome. The batter and the game state could dictate the result of a pitch. Even with the low accuracy, it still performed better than simply choosing the most common accuracy, being statistically significant from the no information rate accuracy.

Spin rate is also an important variable in the search for the best pitchers. Our regression model for predicting spin rate showed that we could accurately train a model built to predict spin rates for only one pitcher, but applying that model to other samples showed the model's weakness. What was especially interesting in our output was how the model tended to under predict spin rates. The model was trained on arguably the best pitcher in the majors from that year, but it still showed that deGrom did not rely on simply having the highest spin rates. The model shows the nuance in what makes a pitcher great, and that looking at purely sensor data about a pitch can never give a full picture about that pitchers impact on winning. 

When examining the differences in outcomes for our 25 pitchers deemed 'elite' vs the rest of the league, we found little differences in average velocity, but the elite pitchers had a higher average break rate of 6.86 compared to 6.6 and 1680 average spin rate compared to 1668 indicating break and spin may be more useful in evaluating a pitcher's stuff. We found statistically significant differences in K rate between the two groups with the elite pitchers having a near 5% higher K rate (4.866%) and a lower walk rate by 1.471%. 

Zack Greinke is a very unique pitcher, as he doesn't have overpowering stuff but has nasty off-speed pitches that in this season particular, was very hard to hit, particularly his changeup and curveball.  The heat map shows he loved throwing his changeup down and in, the curveball thrown all over the lower part of the zone and below.  The 2-seam he loved attacking the inside of the plate although not throwing it much, almost all against right handed batters too. The eephus, unsurprisingly has the most variation of all pitch types, only used 79 times the entire season but is nearly impossible to see where it's going to land. The slider was almost exclusively thrown to the outside corner and the 4-seam fastball located all over the zone which accounted for 43% of his total pitches. From the velocity of his pitch type by inning, we don't see any real change/decrease as the game progresses, indicating that he does not let fatigue affect his stuff. It was a bit surprising to see his two-seam fastball be faster than his four-seam. Additionally, his eephus had an average velocity under 67 mph, making it one if, if not the slowest pitches thrown in the entire league (excluding position players).


## V. Limitations / Recommendations

One way this study was limited was in time and scope. The dataset used has many more variables and relationships between variables than were used within this study. There are many more things to pursue in this project. For instance, as mentioned in our proposal, “what parts of the zone are the lowest xBA for breaking balls & offspeed pitches?” This was one question we did not have time to address and is a possible future use case. 

Another limitation of this project was in terms of computational power. For the classification tree machine learning method, we ended up using half the variables that we had access to. To be fair, there were several variables that wouldn’t have been useful for this step, such as game id, but others would have possibly been useful. With the large amount of data present, it took around 20 to 30 minutes to create a full classification tree. This drastically cut down how extensively we could explore different possible classification trees, as each run would take a significant amount of time. 

With KNN, a large limitation was being unable to use categorical data. Having access to both the game state and the specific batters would give likely much better predictions for the result of the pitch. For this, we recommend using a classification tree, as these allow use of categorical variables. 

Another limitation was that the dataset did not differentiate between starting pitchers and relievers. If possible, we probably would have included only starters as relievers can come in and throw hard not worrying about having to last long. For example, our non elite dataset had a higher average velocity than the elite dataset, due in part to the fact that the elite pitchers are all starters and non elite data includes all the relievers in the league and relievers always account for the top velocities in the league. 
